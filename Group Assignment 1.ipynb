{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c8b0e9",
   "metadata": {},
   "source": [
    "# Assignment 1: American Express - Default Prediction\n",
    "\n",
    "#### Ankita Kokkera - 06032419\n",
    "#### Aria Wang - 06047688\n",
    "#### Tsamara Esperanti Erwin - 06042275\n",
    "#### Jean-Marc Yao - 06055972\n",
    "#### Amer Mulla - 06027165\n",
    "\n",
    "## Introduction to Deep Learning & GenAI\n",
    "\n",
    "The AmEx Credit Default Prediction challenge was a competition to predict the probability that a customer does not pay back their credit card balance amount in the future based on their monthly customer profile. The target binary variable is calculated by observing 18 months performance window after the latest credit card statement, and if the customer does not pay due amount in 120 days after their latest statement date it is considered a default event.\n",
    "\n",
    "In this assignment, you will tackle this challenge using some deep learning models. You will:\n",
    "- Train a neural network using Keras\n",
    "- Understand ERM via loss functions and metrics\n",
    "- Explore learning rates\n",
    "- Implement a cost-sensitive loss\n",
    "- Evaluate business cost via thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa55644",
   "metadata": {},
   "source": [
    "I have preprocessed the data to simplify the challenge, reducing the number of features and samples, and imputing missing values. If you are interested, you can view the original dataset and challenge at https://www.kaggle.com/competitions/amex-default-prediction/\n",
    "\n",
    "For this assignment you are given preprocessed datasets:\n",
    "- `X_train_top50.csv`\n",
    "- `X_test_top50.csv`\n",
    "- `y_train.csv`\n",
    "- `y_test.csv`\n",
    "\n",
    "Follow the hints in each code‐block title to fill in the missing parts of each function, then run each code-block to complete the assignment. Ensure your virtual environment is activated before launching the Jupyter Notebook. For detailed instructions, see `environmentsetup.pdf`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d7562a-6f85-4fd1-85d4-7eeb4dfa7e08",
   "metadata": {},
   "source": [
    "## Installing the Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8bd5de6-c76e-455c-9de2-f2fcae65d9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from pandas) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.24.1 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from scikit-learn) (2.4.1)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from scikit-learn) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.3.0 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: tensorflow in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (2.20.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from tensorflow) (25.12.19)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from tensorflow) (0.7.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from tensorflow) (6.33.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from tensorflow) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from tensorflow) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from tensorflow) (2.4.1)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from tensorflow) (3.15.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (2026.1.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from tensorboard~=2.20.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: pillow in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from tensorboard~=2.20.0->tensorflow) (12.1.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.5)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
      "Requirement already satisfied: namex in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
      "Requirement already satisfied: pandas in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (2.3.3)\n",
      "Requirement already satisfied: scikit-learn in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (1.8.0)\n",
      "Requirement already satisfied: matplotlib in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (3.10.8)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from pandas) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from scikit-learn) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.3.0 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from matplotlib) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from matplotlib) (3.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ankitakokkera/DeepLearning_2026/DL/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install tensorflow\n",
    "!pip install pandas scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206f80d8",
   "metadata": {},
   "source": [
    "## 1. Load the data\n",
    "\n",
    "Load the datasets. Print their shapes and show the first few rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c203ffab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 50) (10000, 50)\n",
      "(50000,) (10000,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B_16_last</th>\n",
       "      <th>B_18_last</th>\n",
       "      <th>B_18_mean</th>\n",
       "      <th>B_18_min</th>\n",
       "      <th>B_20_last</th>\n",
       "      <th>B_22_max</th>\n",
       "      <th>B_23_last</th>\n",
       "      <th>B_2_last</th>\n",
       "      <th>B_2_mean</th>\n",
       "      <th>B_2_min</th>\n",
       "      <th>...</th>\n",
       "      <th>P_2_last</th>\n",
       "      <th>P_2_max</th>\n",
       "      <th>P_2_mean</th>\n",
       "      <th>P_2_min</th>\n",
       "      <th>R_10_max</th>\n",
       "      <th>R_10_std</th>\n",
       "      <th>R_1_last</th>\n",
       "      <th>R_1_max</th>\n",
       "      <th>R_1_mean</th>\n",
       "      <th>R_1_std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customer_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fbac11a8ed792feb62a</th>\n",
       "      <td>0.006410</td>\n",
       "      <td>1.0080</td>\n",
       "      <td>0.8430</td>\n",
       "      <td>0.6460</td>\n",
       "      <td>0.007630</td>\n",
       "      <td>0.009950</td>\n",
       "      <td>0.04037</td>\n",
       "      <td>1.0080</td>\n",
       "      <td>1.0050</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9346</td>\n",
       "      <td>0.9604</td>\n",
       "      <td>0.9336</td>\n",
       "      <td>0.8687</td>\n",
       "      <td>0.008340</td>\n",
       "      <td>0.002386</td>\n",
       "      <td>0.006104</td>\n",
       "      <td>0.009224</td>\n",
       "      <td>0.004510</td>\n",
       "      <td>0.003081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000fd6641609c6ece5454664794f0340ad84dddce9a267a310b5ae68e9d8e5</th>\n",
       "      <td>0.002940</td>\n",
       "      <td>1.0040</td>\n",
       "      <td>1.0050</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.004320</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.01470</td>\n",
       "      <td>1.0040</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.8200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8804</td>\n",
       "      <td>0.9290</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.8613</td>\n",
       "      <td>0.008920</td>\n",
       "      <td>0.002387</td>\n",
       "      <td>0.006912</td>\n",
       "      <td>0.008995</td>\n",
       "      <td>0.006245</td>\n",
       "      <td>0.002128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00001b22f846c82c51f6e3958ccd81970162bae8b007e80662ef27519fcc18c1</th>\n",
       "      <td>0.007835</td>\n",
       "      <td>1.0040</td>\n",
       "      <td>0.9330</td>\n",
       "      <td>0.6895</td>\n",
       "      <td>0.002834</td>\n",
       "      <td>0.008050</td>\n",
       "      <td>0.02023</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.8154</td>\n",
       "      <td>0.8110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8810</td>\n",
       "      <td>0.9043</td>\n",
       "      <td>0.8784</td>\n",
       "      <td>0.7980</td>\n",
       "      <td>0.009895</td>\n",
       "      <td>0.003445</td>\n",
       "      <td>0.006450</td>\n",
       "      <td>0.009445</td>\n",
       "      <td>0.006622</td>\n",
       "      <td>0.001920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000041bdba6ecadd89a52d11886e8eaaec9325906c9723355abb5ca523658edc</th>\n",
       "      <td>0.089200</td>\n",
       "      <td>1.0070</td>\n",
       "      <td>0.9106</td>\n",
       "      <td>0.5440</td>\n",
       "      <td>0.008560</td>\n",
       "      <td>0.009890</td>\n",
       "      <td>0.00506</td>\n",
       "      <td>1.0060</td>\n",
       "      <td>0.9550</td>\n",
       "      <td>0.8120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6216</td>\n",
       "      <td>0.6235</td>\n",
       "      <td>0.5990</td>\n",
       "      <td>0.5674</td>\n",
       "      <td>0.009660</td>\n",
       "      <td>0.002659</td>\n",
       "      <td>0.007830</td>\n",
       "      <td>0.009920</td>\n",
       "      <td>0.005665</td>\n",
       "      <td>0.003473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad51ca8b8c4a24cefed</th>\n",
       "      <td>0.005096</td>\n",
       "      <td>0.5312</td>\n",
       "      <td>0.6100</td>\n",
       "      <td>0.5290</td>\n",
       "      <td>0.008804</td>\n",
       "      <td>0.009730</td>\n",
       "      <td>0.14530</td>\n",
       "      <td>0.8160</td>\n",
       "      <td>0.8145</td>\n",
       "      <td>0.8105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8720</td>\n",
       "      <td>0.9404</td>\n",
       "      <td>0.8916</td>\n",
       "      <td>0.8050</td>\n",
       "      <td>0.008960</td>\n",
       "      <td>0.002479</td>\n",
       "      <td>0.001247</td>\n",
       "      <td>0.009080</td>\n",
       "      <td>0.004180</td>\n",
       "      <td>0.002581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    B_16_last  B_18_last  \\\n",
       "customer_ID                                                                \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...   0.006410     1.0080   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...   0.002940     1.0040   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...   0.007835     1.0040   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...   0.089200     1.0070   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...   0.005096     0.5312   \n",
       "\n",
       "                                                    B_18_mean  B_18_min  \\\n",
       "customer_ID                                                               \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...     0.8430    0.6460   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...     1.0050    1.0000   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...     0.9330    0.6895   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...     0.9106    0.5440   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...     0.6100    0.5290   \n",
       "\n",
       "                                                    B_20_last  B_22_max  \\\n",
       "customer_ID                                                               \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...   0.007630  0.009950   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...   0.004320  0.009766   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...   0.002834  0.008050   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...   0.008560  0.009890   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...   0.008804  0.009730   \n",
       "\n",
       "                                                    B_23_last  B_2_last  \\\n",
       "customer_ID                                                               \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...    0.04037    1.0080   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...    0.01470    1.0040   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...    0.02023    0.8125   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...    0.00506    1.0060   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...    0.14530    0.8160   \n",
       "\n",
       "                                                    B_2_mean  B_2_min  ...  \\\n",
       "customer_ID                                                            ...   \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...    1.0050   1.0000  ...   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...    0.9910   0.8200  ...   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...    0.8154   0.8110  ...   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...    0.9550   0.8120  ...   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...    0.8145   0.8105  ...   \n",
       "\n",
       "                                                    P_2_last  P_2_max  \\\n",
       "customer_ID                                                             \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...    0.9346   0.9604   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...    0.8804   0.9290   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...    0.8810   0.9043   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...    0.6216   0.6235   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...    0.8720   0.9404   \n",
       "\n",
       "                                                    P_2_mean  P_2_min  \\\n",
       "customer_ID                                                             \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...    0.9336   0.8687   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...    0.9000   0.8613   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...    0.8784   0.7980   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...    0.5990   0.5674   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...    0.8916   0.8050   \n",
       "\n",
       "                                                    R_10_max  R_10_std  \\\n",
       "customer_ID                                                              \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...  0.008340  0.002386   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...  0.008920  0.002387   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...  0.009895  0.003445   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...  0.009660  0.002659   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...  0.008960  0.002479   \n",
       "\n",
       "                                                    R_1_last   R_1_max  \\\n",
       "customer_ID                                                              \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...  0.006104  0.009224   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...  0.006912  0.008995   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...  0.006450  0.009445   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...  0.007830  0.009920   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...  0.001247  0.009080   \n",
       "\n",
       "                                                    R_1_mean   R_1_std  \n",
       "customer_ID                                                             \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...  0.004510  0.003081  \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...  0.006245  0.002128  \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...  0.006622  0.001920  \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...  0.005665  0.003473  \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...  0.004180  0.002581  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Read in the datasets, using 'customer_ID' as the index column.\n",
    "X_train = pd.read_csv('data/X_train_top50.csv', index_col='customer_ID')\n",
    "X_test = pd.read_csv('data/X_test_top50.csv', index_col='customer_ID')\n",
    "y_train = pd.read_csv('data/y_train.csv', index_col='customer_ID').values.ravel()\n",
    "y_test = pd.read_csv('data/y_test.csv', index_col='customer_ID').values.ravel()\n",
    "\n",
    "#Display the shapes of the datasets and the first few rows of the training features.\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d94b54",
   "metadata": {},
   "source": [
    "## 2. Feature scaling\n",
    "\n",
    "Neural networks are sensitive to feature scales. Standardize the features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efbe99bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f8c9db",
   "metadata": {},
   "source": [
    "## 4. Baseline Model: Logistic Regression\n",
    "\n",
    "Build and train logistic regression as an off-the-shelf baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "626ec3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model1 = LogisticRegression()\n",
    "model1 = model1.fit(X_train_scaled, y_train)\n",
    "\n",
    "preds = model1.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99796bfe",
   "metadata": {},
   "source": [
    "As we discussed in lecture, making incorrect predictions could incur different associated costs. Assume:\n",
    "- False negatives (default) cost £50,000.\n",
    "- False positives (no loan) cost £5,000.\n",
    "\n",
    "Let's write an economic performance metric to evaluate models using these costs. Below is the skeleton to compute the cost incurred by our model predictions over our testing dataset. **Do not** change any existing lines, only fill in the sections marked: \n",
    "```\n",
    "############## YOUR CODE HERE ##############\n",
    "#                                           \n",
    "#                                           \n",
    "############## YOUR CODE HERE ##############\n",
    "```\n",
    "Some hints are provided above the marked sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbc0e820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data: 1861 correct out of 2551 positives (accuracy 72.95%)\n",
      "Test data: 6954 correct out of 7449 negatives (accuracy 93.35%)\n",
      "TOTAL COST: £36975000.00\n"
     ]
    }
   ],
   "source": [
    "def evaluate_cost(preds, y_test):\n",
    "\n",
    "    numpos = 0; numpos_correct = 0\n",
    "    numneg = 0; numneg_correct = 0\n",
    "\n",
    "    for (pred, output) in zip(preds,y_test):\n",
    "        if output == 1: \n",
    "            numpos += 1\n",
    "            if pred >= 1: numpos_correct += 1\n",
    "        if output == 0: \n",
    "            numneg += 1\n",
    "            if pred == 0: numneg_correct += 1\n",
    "\n",
    "    accpos = numpos_correct / max(1,numpos); accneg = numneg_correct / max(1,numneg)\n",
    "\n",
    "    # Count occurrences of false positives and false negatives\n",
    "    # From these, compute the total economic cost\n",
    "    \n",
    "    ############## YOUR CODE HERE ##############\n",
    "    \n",
    "    # False Negatives: Actual 1s that were NOT predicted correctly\n",
    "    false_negatives = numpos - numpos_correct \n",
    "    # False Positives: Actual 0s that were NOT predicted correctly\n",
    "    false_positives = numneg - numneg_correct\n",
    "    # If a specific cost per instance isn't provided, a standard sum might look like:\n",
    "    totalcost = (false_negatives * 50000.0) + (false_positives * 5000.0)\n",
    "    \n",
    "    ############## YOUR CODE HERE ##############\n",
    "\n",
    "    print(f'Test data: {numpos_correct} correct out of {numpos} positives (accuracy {accpos*100:.2f}%)')\n",
    "    print(f'Test data: {numneg_correct} correct out of {numneg} negatives (accuracy {accneg*100:.2f}%)')\n",
    "    print(f'TOTAL COST: £{(totalcost):.2f}')\n",
    "\n",
    "evaluate_cost(preds, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f1550a",
   "metadata": {},
   "source": [
    "## 4. Baseline Neural Network (Binary Cross-Entropy)\n",
    "\n",
    "Build and train a baseline model using binary cross-entropy.\n",
    "\n",
    "**Tasks:**\n",
    "1. Build the model\n",
    "2. Train it\n",
    "3. Plot training/validation loss\n",
    "4. Report accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03e853f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7928 - loss: 0.4458 - val_accuracy: 0.8341 - val_loss: 0.3655\n",
      "Epoch 2/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8377 - loss: 0.3461 - val_accuracy: 0.8469 - val_loss: 0.3280\n",
      "Epoch 3/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8459 - loss: 0.3235 - val_accuracy: 0.8506 - val_loss: 0.3141\n",
      "Epoch 4/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8510 - loss: 0.3132 - val_accuracy: 0.8563 - val_loss: 0.3062\n",
      "Epoch 5/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 995us/step - accuracy: 0.8547 - loss: 0.3068 - val_accuracy: 0.8586 - val_loss: 0.3008\n",
      "Epoch 6/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 982us/step - accuracy: 0.8572 - loss: 0.3021 - val_accuracy: 0.8617 - val_loss: 0.2965\n",
      "Epoch 7/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 989us/step - accuracy: 0.8598 - loss: 0.2984 - val_accuracy: 0.8640 - val_loss: 0.2932\n",
      "Epoch 8/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8613 - loss: 0.2953 - val_accuracy: 0.8644 - val_loss: 0.2903\n",
      "Epoch 9/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8630 - loss: 0.2926 - val_accuracy: 0.8655 - val_loss: 0.2878\n",
      "Epoch 10/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8642 - loss: 0.2904 - val_accuracy: 0.8677 - val_loss: 0.2857\n",
      "Epoch 11/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8653 - loss: 0.2884 - val_accuracy: 0.8678 - val_loss: 0.2842\n",
      "Epoch 12/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8672 - loss: 0.2867 - val_accuracy: 0.8692 - val_loss: 0.2825\n",
      "Epoch 13/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8678 - loss: 0.2852 - val_accuracy: 0.8699 - val_loss: 0.2812\n",
      "Epoch 14/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8689 - loss: 0.2839 - val_accuracy: 0.8708 - val_loss: 0.2799\n",
      "Epoch 15/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8696 - loss: 0.2827 - val_accuracy: 0.8709 - val_loss: 0.2789\n",
      "Epoch 16/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8705 - loss: 0.2817 - val_accuracy: 0.8710 - val_loss: 0.2780\n",
      "Epoch 17/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8713 - loss: 0.2807 - val_accuracy: 0.8720 - val_loss: 0.2770\n",
      "Epoch 18/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8718 - loss: 0.2799 - val_accuracy: 0.8734 - val_loss: 0.2762\n",
      "Epoch 19/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8724 - loss: 0.2791 - val_accuracy: 0.8733 - val_loss: 0.2756\n",
      "Epoch 20/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8728 - loss: 0.2784 - val_accuracy: 0.8733 - val_loss: 0.2751\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "#Here we define a simple feedforward neural network model. The architecture is set in the lines below.\n",
    "def build_model(input_dim):\n",
    "    model2 = models.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model2\n",
    "\n",
    "model2 = build_model(X_train_scaled.shape[1])\n",
    "model2.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "history = model2.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=20,\n",
    "    batch_size=500,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8829649f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334us/step\n",
      "Test data: 1876 correct out of 2551 positives (accuracy 73.54%)\n",
      "Test data: 6875 correct out of 7449 negatives (accuracy 92.29%)\n",
      "TOTAL COST: £36620000.00\n"
     ]
    }
   ],
   "source": [
    "#Let's evaluate the neural network model on the test data.\n",
    "nn_preds_prob = model2.predict(X_test_scaled)   \n",
    "nn_preds = (nn_preds_prob >= 0.5).astype(int).ravel()\n",
    "\n",
    "evaluate_cost(nn_preds, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea8b11d",
   "metadata": {},
   "source": [
    "How does the accuracy compare to that of the logistic regression model? How does the economic performance metric compare?\n",
    "\n",
    "- We observe that the Neural Network achieves slightly higher accuracy than the Logistic Regression model because it can capture non-linear relationships between features that a linear model may fail to detect.\n",
    "\n",
    "- In terms of economic performance, accuracy alone can be misleading in finance. A False Negative (failing to identify a default) is significantly more costly than a False Positive (incorrectly rejecting a reliable customer). Therefore, the Total Cost computed using the evaluate_cost function is a more meaningful metric for business decision-making.\n",
    "\n",
    "- Both models achieve similar classification accuracy on the test data. Logistic regression performs slightly better in identifying negatives, while the neural network achieves marginally higher accuracy on positives. However, when comparing the economic performance metric, the Neural Network results in a lower overall cost (£36.975M vs £36.62M). This suggests that its improved detection of positive cases outweighs the increase in false positives, making it the more cost-effective model.\n",
    "\n",
    "### (Bonus): Try playing with the model to get a higher accuracy!\n",
    "\n",
    "To improve the neural network's accuracy, we could try by increasing the **model capacity** by adding more neurons to the existing layers or introducing additional hidden layers to help the network capture more complex, non-linear patterns in the credit data. Furthermore, increasing the number of **epochs** from 20 to a higher value like 50 or 100 would also allow the model more time to learn, provided we monitor the validation loss to prevent overfitting. Finally, fine-tuning the **learning rate** or adjusting the **batch size** can provide more granular control over the optimization process, often leading to better generalization to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aff8243-6802-42ac-b23d-cf4ec872e6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8434 - loss: 0.3852 - val_accuracy: 0.8618 - val_loss: 0.3183\n",
      "Epoch 2/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8595 - loss: 0.3093 - val_accuracy: 0.8669 - val_loss: 0.2977\n",
      "Epoch 3/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8651 - loss: 0.2957 - val_accuracy: 0.8714 - val_loss: 0.2878\n",
      "Epoch 4/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8693 - loss: 0.2880 - val_accuracy: 0.8732 - val_loss: 0.2823\n",
      "Epoch 5/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8720 - loss: 0.2829 - val_accuracy: 0.8745 - val_loss: 0.2785\n",
      "Epoch 6/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8725 - loss: 0.2794 - val_accuracy: 0.8764 - val_loss: 0.2755\n",
      "Epoch 7/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8746 - loss: 0.2769 - val_accuracy: 0.8768 - val_loss: 0.2745\n",
      "Epoch 8/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8753 - loss: 0.2751 - val_accuracy: 0.8779 - val_loss: 0.2726\n",
      "Epoch 9/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8761 - loss: 0.2737 - val_accuracy: 0.8775 - val_loss: 0.2718\n",
      "Epoch 10/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8769 - loss: 0.2724 - val_accuracy: 0.8791 - val_loss: 0.2701\n",
      "Epoch 11/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8772 - loss: 0.2716 - val_accuracy: 0.8793 - val_loss: 0.2694\n",
      "Epoch 12/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8776 - loss: 0.2707 - val_accuracy: 0.8792 - val_loss: 0.2689\n",
      "Epoch 13/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8783 - loss: 0.2700 - val_accuracy: 0.8801 - val_loss: 0.2681\n",
      "Epoch 14/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8790 - loss: 0.2693 - val_accuracy: 0.8792 - val_loss: 0.2682\n",
      "Epoch 15/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8793 - loss: 0.2688 - val_accuracy: 0.8803 - val_loss: 0.2673\n",
      "Epoch 16/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8789 - loss: 0.2683 - val_accuracy: 0.8799 - val_loss: 0.2671\n",
      "Epoch 17/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8799 - loss: 0.2678 - val_accuracy: 0.8799 - val_loss: 0.2670\n",
      "Epoch 18/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8797 - loss: 0.2672 - val_accuracy: 0.8790 - val_loss: 0.2673\n",
      "Epoch 19/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8798 - loss: 0.2670 - val_accuracy: 0.8797 - val_loss: 0.2675\n",
      "Epoch 20/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8801 - loss: 0.2666 - val_accuracy: 0.8811 - val_loss: 0.2658\n",
      "Epoch 21/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8804 - loss: 0.2662 - val_accuracy: 0.8803 - val_loss: 0.2663\n",
      "Epoch 22/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8809 - loss: 0.2658 - val_accuracy: 0.8792 - val_loss: 0.2658\n",
      "Epoch 23/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8807 - loss: 0.2655 - val_accuracy: 0.8810 - val_loss: 0.2653\n",
      "Epoch 24/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8810 - loss: 0.2652 - val_accuracy: 0.8815 - val_loss: 0.2650\n",
      "Epoch 25/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8807 - loss: 0.2650 - val_accuracy: 0.8814 - val_loss: 0.2648\n",
      "Epoch 26/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8807 - loss: 0.2647 - val_accuracy: 0.8792 - val_loss: 0.2660\n",
      "Epoch 27/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8812 - loss: 0.2643 - val_accuracy: 0.8813 - val_loss: 0.2648\n",
      "Epoch 28/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8809 - loss: 0.2641 - val_accuracy: 0.8810 - val_loss: 0.2649\n",
      "Epoch 29/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8806 - loss: 0.2638 - val_accuracy: 0.8818 - val_loss: 0.2642\n",
      "Epoch 30/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8813 - loss: 0.2636 - val_accuracy: 0.8804 - val_loss: 0.2644\n",
      "Epoch 31/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8815 - loss: 0.2634 - val_accuracy: 0.8795 - val_loss: 0.2658\n",
      "Epoch 32/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8817 - loss: 0.2631 - val_accuracy: 0.8801 - val_loss: 0.2647\n",
      "Epoch 33/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8817 - loss: 0.2629 - val_accuracy: 0.8804 - val_loss: 0.2647\n",
      "Epoch 34/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8814 - loss: 0.2627 - val_accuracy: 0.8817 - val_loss: 0.2638\n",
      "Epoch 35/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8813 - loss: 0.2625 - val_accuracy: 0.8811 - val_loss: 0.2650\n",
      "Epoch 36/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8815 - loss: 0.2623 - val_accuracy: 0.8807 - val_loss: 0.2647\n",
      "Epoch 37/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8815 - loss: 0.2621 - val_accuracy: 0.8818 - val_loss: 0.2633\n",
      "Epoch 38/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8821 - loss: 0.2619 - val_accuracy: 0.8819 - val_loss: 0.2632\n",
      "Epoch 39/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8822 - loss: 0.2617 - val_accuracy: 0.8812 - val_loss: 0.2632\n",
      "Epoch 40/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8824 - loss: 0.2614 - val_accuracy: 0.8816 - val_loss: 0.2630\n",
      "Epoch 41/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8824 - loss: 0.2613 - val_accuracy: 0.8820 - val_loss: 0.2629\n",
      "Epoch 42/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8820 - loss: 0.2611 - val_accuracy: 0.8821 - val_loss: 0.2627\n",
      "Epoch 43/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8821 - loss: 0.2610 - val_accuracy: 0.8807 - val_loss: 0.2634\n",
      "Epoch 44/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8823 - loss: 0.2607 - val_accuracy: 0.8820 - val_loss: 0.2627\n",
      "Epoch 45/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8825 - loss: 0.2606 - val_accuracy: 0.8807 - val_loss: 0.2639\n",
      "Epoch 46/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8824 - loss: 0.2604 - val_accuracy: 0.8811 - val_loss: 0.2628\n",
      "Epoch 47/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8826 - loss: 0.2603 - val_accuracy: 0.8806 - val_loss: 0.2635\n",
      "Epoch 48/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8822 - loss: 0.2601 - val_accuracy: 0.8809 - val_loss: 0.2633\n",
      "Epoch 49/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8830 - loss: 0.2598 - val_accuracy: 0.8810 - val_loss: 0.2639\n",
      "Epoch 50/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8830 - loss: 0.2598 - val_accuracy: 0.8812 - val_loss: 0.2626\n",
      "Epoch 51/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8832 - loss: 0.2596 - val_accuracy: 0.8807 - val_loss: 0.2629\n",
      "Epoch 52/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8832 - loss: 0.2594 - val_accuracy: 0.8823 - val_loss: 0.2621\n",
      "Epoch 53/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8832 - loss: 0.2593 - val_accuracy: 0.8830 - val_loss: 0.2619\n",
      "Epoch 54/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8833 - loss: 0.2590 - val_accuracy: 0.8830 - val_loss: 0.2620\n",
      "Epoch 55/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8835 - loss: 0.2590 - val_accuracy: 0.8828 - val_loss: 0.2619\n",
      "Epoch 56/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8841 - loss: 0.2588 - val_accuracy: 0.8824 - val_loss: 0.2623\n",
      "Epoch 57/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8835 - loss: 0.2587 - val_accuracy: 0.8832 - val_loss: 0.2617\n",
      "Epoch 58/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8841 - loss: 0.2585 - val_accuracy: 0.8826 - val_loss: 0.2621\n",
      "Epoch 59/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8841 - loss: 0.2583 - val_accuracy: 0.8819 - val_loss: 0.2628\n",
      "Epoch 60/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8841 - loss: 0.2582 - val_accuracy: 0.8827 - val_loss: 0.2615\n",
      "Epoch 61/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8836 - loss: 0.2580 - val_accuracy: 0.8824 - val_loss: 0.2616\n",
      "Epoch 62/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8839 - loss: 0.2579 - val_accuracy: 0.8823 - val_loss: 0.2618\n",
      "Epoch 63/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8846 - loss: 0.2578 - val_accuracy: 0.8823 - val_loss: 0.2620\n",
      "Epoch 64/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8845 - loss: 0.2577 - val_accuracy: 0.8830 - val_loss: 0.2614\n",
      "Epoch 65/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8845 - loss: 0.2576 - val_accuracy: 0.8827 - val_loss: 0.2622\n",
      "Epoch 66/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8847 - loss: 0.2573 - val_accuracy: 0.8829 - val_loss: 0.2614\n",
      "Epoch 67/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8845 - loss: 0.2572 - val_accuracy: 0.8830 - val_loss: 0.2616\n",
      "Epoch 68/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8846 - loss: 0.2572 - val_accuracy: 0.8825 - val_loss: 0.2618\n",
      "Epoch 69/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8850 - loss: 0.2569 - val_accuracy: 0.8827 - val_loss: 0.2620\n",
      "Epoch 70/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8851 - loss: 0.2568 - val_accuracy: 0.8835 - val_loss: 0.2622\n",
      "Epoch 71/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8851 - loss: 0.2567 - val_accuracy: 0.8826 - val_loss: 0.2618\n",
      "Epoch 72/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8848 - loss: 0.2566 - val_accuracy: 0.8833 - val_loss: 0.2611\n",
      "Epoch 73/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8852 - loss: 0.2565 - val_accuracy: 0.8838 - val_loss: 0.2618\n",
      "Epoch 74/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8853 - loss: 0.2563 - val_accuracy: 0.8832 - val_loss: 0.2612\n",
      "Epoch 75/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8854 - loss: 0.2563 - val_accuracy: 0.8836 - val_loss: 0.2618\n",
      "Epoch 76/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8860 - loss: 0.2559 - val_accuracy: 0.8834 - val_loss: 0.2616\n",
      "Epoch 77/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8855 - loss: 0.2559 - val_accuracy: 0.8826 - val_loss: 0.2609\n",
      "Epoch 78/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8858 - loss: 0.2559 - val_accuracy: 0.8816 - val_loss: 0.2613\n",
      "Epoch 79/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8859 - loss: 0.2557 - val_accuracy: 0.8829 - val_loss: 0.2616\n",
      "Epoch 80/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8858 - loss: 0.2555 - val_accuracy: 0.8828 - val_loss: 0.2610\n",
      "Epoch 81/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8857 - loss: 0.2554 - val_accuracy: 0.8831 - val_loss: 0.2610\n",
      "Epoch 82/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8857 - loss: 0.2553 - val_accuracy: 0.8827 - val_loss: 0.2608\n",
      "Epoch 83/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8861 - loss: 0.2552 - val_accuracy: 0.8826 - val_loss: 0.2609\n",
      "Epoch 84/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8861 - loss: 0.2551 - val_accuracy: 0.8824 - val_loss: 0.2609\n",
      "Epoch 85/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8856 - loss: 0.2549 - val_accuracy: 0.8835 - val_loss: 0.2613\n",
      "Epoch 86/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8862 - loss: 0.2548 - val_accuracy: 0.8829 - val_loss: 0.2610\n",
      "Epoch 87/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8861 - loss: 0.2547 - val_accuracy: 0.8833 - val_loss: 0.2615\n",
      "Epoch 88/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8864 - loss: 0.2546 - val_accuracy: 0.8822 - val_loss: 0.2607\n",
      "Epoch 89/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8863 - loss: 0.2545 - val_accuracy: 0.8830 - val_loss: 0.2608\n",
      "Epoch 90/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8866 - loss: 0.2544 - val_accuracy: 0.8835 - val_loss: 0.2620\n",
      "Epoch 91/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8864 - loss: 0.2543 - val_accuracy: 0.8836 - val_loss: 0.2619\n",
      "Epoch 92/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8863 - loss: 0.2541 - val_accuracy: 0.8833 - val_loss: 0.2625\n",
      "Epoch 93/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8865 - loss: 0.2540 - val_accuracy: 0.8828 - val_loss: 0.2605\n",
      "Epoch 94/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8865 - loss: 0.2538 - val_accuracy: 0.8823 - val_loss: 0.2628\n",
      "Epoch 95/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8868 - loss: 0.2538 - val_accuracy: 0.8824 - val_loss: 0.2610\n",
      "Epoch 96/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8867 - loss: 0.2537 - val_accuracy: 0.8830 - val_loss: 0.2605\n",
      "Epoch 97/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8869 - loss: 0.2536 - val_accuracy: 0.8814 - val_loss: 0.2605\n",
      "Epoch 98/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8871 - loss: 0.2534 - val_accuracy: 0.8820 - val_loss: 0.2605\n",
      "Epoch 99/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8870 - loss: 0.2533 - val_accuracy: 0.8828 - val_loss: 0.2610\n",
      "Epoch 100/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8870 - loss: 0.2533 - val_accuracy: 0.8819 - val_loss: 0.2606\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 380us/step\n",
      "Test data: 1940 correct out of 2551 positives (accuracy 76.05%)\n",
      "Test data: 6888 correct out of 7449 negatives (accuracy 92.47%)\n",
      "TOTAL COST: £33355000.00\n"
     ]
    }
   ],
   "source": [
    "# Improved Neural Network Architecture\n",
    "def build_higher_accuracy_model(input_dim):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(128, activation='relu'),   # Increased initial layer size\n",
    "        layers.Dense(64, activation='relu'),    # Added an extra hidden layer\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Re-building the model with the training data dimensions\n",
    "model2_improved = build_higher_accuracy_model(X_train_scaled.shape[1])\n",
    "\n",
    "# Keeping the 'sgd' optimizer but increasing the training duration\n",
    "model2_improved.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "history = model2_improved.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,           # Increased epochs to allow SGD more time to converge\n",
    "    batch_size=256,       # Slightly smaller batch size can provide more frequent weight updates\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluating the improved model\n",
    "nn_preds_prob_new = model2_improved.predict(X_test_scaled)\n",
    "nn_preds_new = (nn_preds_prob_new >= 0.5).astype(int).ravel()\n",
    "\n",
    "evaluate_cost(nn_preds_new, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dc05ab-714b-4f74-a164-ffe201607a0a",
   "metadata": {},
   "source": [
    "We can see that the accuracy has increased from 73.54 to 76.05. The model’s accuracy fluctuates and generally improves across different runs due to stochasticity. Every time the model is initialized, its internal weights are set to different small random numbers, meaning the neural network begins its optimization journey from a unique starting point in each session. During the training phase, the Stochastic Gradient Descent (SGD) optimizer selects random batches of data, in this case a batch_size of 256, to calculate weight updates. Consequently, the model sees and reacts to the customer profile data in a slightly different order during every run.\n",
    "\n",
    "By increasing the training to 100 epochs and adding more layers (with 128 and 64 neurons), the model gained the depth needed to understand complex patterns in the data. This extra time and capacity helped the optimizer move past local minima to find a better set of weights and achieve higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec66a21c",
   "metadata": {},
   "source": [
    "## 4. Learning Rate Experiments\n",
    "\n",
    "Below we train the same model with a few different learning rates.\n",
    "How do these affect stability and convergence?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c702276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with learning rate = 0.001\n",
      "Training with learning rate = 0.01\n",
      "Training with learning rate = 0.1\n",
      "Training with learning rate = 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "learning_rates = [1e-3, 1e-2, 1e-1, 1.0]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f'Training with learning rate = {lr}')\n",
    "    model2 = build_model(X_train_scaled.shape[1])\n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "    model2.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    history = model2.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=20,\n",
    "        batch_size=500,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    plt.plot(history.history['val_loss'], label=f'lr={lr}')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894b1fca-7f98-46de-8bfb-ec1fb5614086",
   "metadata": {},
   "source": [
    "When using Stochastic Gradient Descent (SGD), the learning rate is the most critical hyperparameter because it controls the size of the steps the model takes toward the minimum of the loss function. In the results, the learning rate of **0.001** is too conservative. While the loss curve is very smooth and stable, indicating that the model is consistently moving in the right direction, the descent is slow that it remains far from the optimal solution after 20 epochs. This creates a risk of underfitting if the training budget is limited.\n",
    "\n",
    "The learning rate of **0.01** shows a much healthier balance for a standard training run. It achieves a significantly lower validation loss much faster than the smaller rate while maintaining perfect stability. This rate allows the model to capture the general trend of the data without being distracted by noise. However, looking at the graph, it still hasn't quite reached the plateau that the higher rates achieved, suggesting it could still benefit from more training time.\n",
    "\n",
    "At **0.1**, we see the most efficient convergence. The model reaches its floor almost immediately, within the first 2.5 to 5 epochs. While there are some very slight ripples in the validation loss, they are negligible compared to the speed gains. In most practical scenarios, this would be considered the ideal rate among our choices because it maximizes the utility of each epoch and reaches a high-performing state quickly.\n",
    "\n",
    "Finally, the learning rate of **1.0** illustrates the danger of setting the rate too high. Although it hits a low loss value instantly, the red line is characterized by jagged oscillations. Because the steps are large, the optimizer is likely overshooting the actual minimum, jumping from one side to the other without settling at the bottom. This lack of stability can prevent the model from reaching the best weights and can sometimes lead to the loss exploding entirely if the rate were any higher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afa675c",
   "metadata": {},
   "source": [
    "## 5. Cost-Sensitive Loss\n",
    "\n",
    "Again, assume:\n",
    "- False negatives (default) cost £50,000.\n",
    "- False positives (no loan) cost £5,000.\n",
    "\n",
    "Implement a loss function that reflects this asymmetric cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0ad3f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7443 - loss: 4869.9238 - val_accuracy: 0.7835 - val_loss: 4072.0784\n",
      "Epoch 2/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7804 - loss: 4032.8621 - val_accuracy: 0.7981 - val_loss: 3920.3333\n",
      "Epoch 3/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7893 - loss: 3925.0979 - val_accuracy: 0.7946 - val_loss: 3865.4839\n",
      "Epoch 4/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7929 - loss: 3876.3521 - val_accuracy: 0.7980 - val_loss: 3843.0237\n",
      "Epoch 5/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7954 - loss: 3845.9524 - val_accuracy: 0.7995 - val_loss: 3859.2031\n",
      "Epoch 6/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7932 - loss: 3824.3840 - val_accuracy: 0.8021 - val_loss: 3834.0996\n",
      "Epoch 7/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7949 - loss: 3806.2224 - val_accuracy: 0.7997 - val_loss: 3821.2939\n",
      "Epoch 8/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7972 - loss: 3794.2085 - val_accuracy: 0.8007 - val_loss: 3822.4724\n",
      "Epoch 9/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7963 - loss: 3781.1775 - val_accuracy: 0.8032 - val_loss: 3799.8379\n",
      "Epoch 10/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7984 - loss: 3761.1191 - val_accuracy: 0.8002 - val_loss: 3812.2708\n",
      "Epoch 11/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7976 - loss: 3757.0208 - val_accuracy: 0.8047 - val_loss: 3795.8628\n",
      "Epoch 12/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7969 - loss: 3742.2317 - val_accuracy: 0.8047 - val_loss: 3788.3984\n",
      "Epoch 13/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7997 - loss: 3734.3069 - val_accuracy: 0.8016 - val_loss: 3784.2512\n",
      "Epoch 14/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7987 - loss: 3725.8645 - val_accuracy: 0.8132 - val_loss: 3817.4033\n",
      "Epoch 15/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8000 - loss: 3716.8735 - val_accuracy: 0.7940 - val_loss: 3779.4631\n",
      "Epoch 16/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7979 - loss: 3709.5439 - val_accuracy: 0.8072 - val_loss: 3787.0383\n",
      "Epoch 17/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7998 - loss: 3704.2295 - val_accuracy: 0.8071 - val_loss: 3785.8965\n",
      "Epoch 18/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8005 - loss: 3695.5061 - val_accuracy: 0.8053 - val_loss: 3771.2549\n",
      "Epoch 19/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8001 - loss: 3680.9541 - val_accuracy: 0.7988 - val_loss: 3810.7644\n",
      "Epoch 20/20\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8011 - loss: 3680.8372 - val_accuracy: 0.7999 - val_loss: 3769.5063\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def cost_sensitive_loss(y_true, y_pred):\n",
    "    # Convert labels to float\n",
    "    y_true = K.cast(y_true, K.floatx())\n",
    "    \n",
    "    # Numerical stability\n",
    "    eps = K.epsilon()\n",
    "    y_pred = K.clip(y_pred, eps, 1 - eps)\n",
    "\n",
    "    # Write your loss function here. Hint, cross-entropy can be written in this syntax as:\n",
    "    # loss = - (y_true * K.log(y_pred) + (1 - y_true) * K.log(1 - y_pred))\n",
    "    \n",
    "    ############## YOUR CODE HERE ##############\n",
    "    \n",
    "    # Weights based on the provided costs\n",
    "    cost_fn = 50000.0  # Cost of missing a default (y_true=1, y_pred=0)\n",
    "    cost_fp = 5000.0   # Cost of a false alarm (y_true=0, y_pred=1)\n",
    "\n",
    "    # Applying the weights to the cross-entropy formula\n",
    "    loss = -(cost_fn * y_true * K.log(y_pred) + cost_fp * (1 - y_true) * K.log(1 - y_pred))\n",
    "    \n",
    "    ############## YOUR CODE HERE ##############\n",
    "    \n",
    "    return K.mean(loss)\n",
    "\n",
    "model3 = build_model(X_train_scaled.shape[1])\n",
    "model3.compile(loss=cost_sensitive_loss, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model3.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=20,\n",
    "    batch_size=500,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9adada49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 331us/step\n",
      "Test data: 2454 correct out of 2551 positives (accuracy 96.20%)\n",
      "Test data: 5555 correct out of 7449 negatives (accuracy 74.57%)\n",
      "TOTAL COST: £14320000.00\n"
     ]
    }
   ],
   "source": [
    "#Let's evaluate the neural network model on the test data.\n",
    "nn_preds_prob = model3.predict(X_test_scaled)   \n",
    "nn_preds = (nn_preds_prob >= 0.5).astype(int).ravel()\n",
    "\n",
    "evaluate_cost(nn_preds, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9143c6",
   "metadata": {},
   "source": [
    "## 7. Reflection\n",
    "\n",
    "In 2-3 sentences each, please answer:\n",
    "- Q1. How does learning rate affect training behavior?\n",
    "- Q2a. Did anything else you tried improve the model accuracy? \n",
    "- Q2b. Why or why not? <br>\n",
    "(**Q2 Note: marks are based on reflection, not on what accuracy you found!**)\n",
    "- Q3. Why is cost-sensitive loss more appropriate here than accuracy?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2f65d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## YOUR RESPONSE HERE ##############\n",
    "#\n",
    "# Question 1:\n",
    "# The learning rate dictates the step size the optimizer takes toward the minimum loss. A very small rate (like 0.001) results in stable but slow convergence that may underfit, while a very high rate (like 1.0) can cause the model to overshoot the minimum, leading to unstable oscillations or failure to converge.\n",
    "\n",
    "# Question 2a:\n",
    "# Beyond adjusting the learning rate, ensuring that the input features were properly normalized through scaling was a key factor in improving performance. We also observed that increasing the number of training epochs allowed the slower learning rates to eventually reach a higher accuracy level that they couldn't achieve in a shorter run.\n",
    "\n",
    "# Question 2b:\n",
    "# Feature scaling is highly effective because it prevents features with larger numerical ranges from dominating the gradient updates, creating a more uniform loss surface that SGD can navigate more directly. Additionally, giving the model more time to train (more epochs) is necessary for smaller learning rates because they take small steps. Without enough iterations, they simply stop before reaching the optimal weights.\n",
    "\n",
    "# Question 3:\n",
    "# Accuracy treats all misclassifications as equal, but in banking, the financial impact of a false negative (missing a default) is ten times higher than a false positive (denying a good loan). A cost-sensitive loss forces the model to prioritize avoiding the most expensive mistakes, ultimately minimizing the total financial loss rather than just maximizing the percentage of correct guesses.\n",
    "\n",
    "############## YOUR RESPONSE HERE ##############"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
